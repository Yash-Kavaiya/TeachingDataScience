{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def elapsed(self, sec):\n",
    "        return str(sec) + \" secs\"\n",
    "    \n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time()-self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \n",
    "    def __init__(self, img_rows = 28, img_cols = 28, channel = 1):\n",
    "        \n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None #Discriminator\n",
    "        self.G = None #Generator\n",
    "        self.AM = None #Adversarial Model\n",
    "        self.DM = None #Discriminator Model\n",
    "        \n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        \n",
    "        # create a keras sequential model\n",
    "        self.D = Sequential()\n",
    "        \n",
    "        # how many filters in my first layer\n",
    "        filters = 64\n",
    "        dropout = 0.4\n",
    "        \n",
    "        # add conv layer 1 - 64 filters, filter size is 5x5 , it jumps 2 pixels at a time\n",
    "        # and it takes in an img which is 28x28x1\n",
    "        self.D.add(Conv2D(filters=filters*1, kernel_size=5, strides=2, input_shape=(self.img_rows, self.img_cols, self.channel),\n",
    "                         padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "        # randomy switch off certain neurons to prevent overfitting\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(filters=filters*2, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(0.2)))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(filters=filters*4, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(0.2)))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(filters=filters*8, kernel_size=5, strides=1, padding='same', activation=LeakyReLU(0.2)))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        # i will have 512 images which are 4x4 pixels\n",
    "        \n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        # it will output a value between 0 and 1 to denote how real the image is\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "    \n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        \n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64*4\n",
    "        dim = 7\n",
    "        \n",
    "        # expect 100 values as input\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        \n",
    "        # reshape them into a matrix\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "        \n",
    "        #outputs twice of previous input\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        \n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        \n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        # now its 28X28\n",
    "        \n",
    "        self.G.add(Conv2DTranspose(1,5,padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "    \n",
    "    # it is converting the architecture of the discriminator into a model\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "    \n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        \n",
    "        optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        # it is adding the two models back to back\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MNIST_DCGAN(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "        \n",
    "        self.x_train = input_data.read_data_sets(\"mnist\", one_hot=True).train.images\n",
    "        \n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n",
    "        \n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator = self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        \n",
    "        self.generator = self.DCGAN.generator()\n",
    "        \n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0,1.0,size=[16,100])\n",
    "            \n",
    "        for i in range(train_steps):\n",
    "            \n",
    "            # it selects 256 random real images from the mnist dataset\n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            # generator 256 random noise values\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            \n",
    "            # generates 256 random images\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            \n",
    "            # add your real and fake images together\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "           \n",
    "            # mark the first 256 as 1 and the next 256 as 0\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:,:] = 0\n",
    "            \n",
    "            \n",
    "            # train the discriminator on these 512 real + fake images\n",
    "            d_loss = self.discriminator.train_on_batch(x,y)\n",
    "            \n",
    "            \n",
    "            # generate 256 new fake images, mark them as 1 and pass them through the network\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size,100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            \n",
    "            if save_interval>0:\n",
    "                \n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0], noise=noise_input, step=(i+1))\n",
    "        \n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename= 'mnist.png'\n",
    "        \n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i,:,:,:]\n",
    "            \n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4,4,i+1)\n",
    "            image = images[i,:,:,:]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "noise = np.random.uniform(-1.0, 1.0, size=[16,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19351976,  0.43048803,  0.21538106, -0.70670492, -0.24176968,\n",
       "       -0.17041408,  0.21306492,  0.52012824,  0.82190431, -0.05543309,\n",
       "        0.77944312,  0.2339292 , -0.75928631,  0.14310142,  0.79867526,\n",
       "        0.5729525 , -0.67927623,  0.46017953,  0.2754592 , -0.62788468,\n",
       "       -0.9967468 , -0.76762488, -0.04542076,  0.03441053, -0.57196576,\n",
       "       -0.84906942,  0.97426391, -0.76840276, -0.90872641, -0.9062041 ,\n",
       "        0.33918072, -0.9567843 , -0.05653281, -0.68545544, -0.06655165,\n",
       "       -0.49247392,  0.95511936,  0.62721712, -0.44092347, -0.2851462 ,\n",
       "       -0.82510984,  0.02276676,  0.82041729, -0.49635846, -0.18785479,\n",
       "        0.79828958,  0.41772887, -0.55441147,  0.90434734,  0.74691212,\n",
       "       -0.48451421, -0.67880935,  0.70755651, -0.83981038,  0.74736398,\n",
       "        0.74071078,  0.8696384 ,  0.38207864,  0.97340059,  0.28763158,\n",
       "       -0.71119975, -0.78375381,  0.18394716,  0.52223724,  0.58767298,\n",
       "       -0.90519096,  0.22012091, -0.60015871,  0.70458814, -0.28245467,\n",
       "       -0.68811566,  0.40691676,  0.56695821,  0.3407485 ,  0.6363087 ,\n",
       "       -0.15277363, -0.23124374,  0.94235075,  0.77956914, -0.37994616,\n",
       "        0.17273848, -0.35332186,  0.75585706,  0.07246529,  0.79613641,\n",
       "        0.56824494, -0.6767984 , -0.17596092, -0.25593973, -0.0166987 ,\n",
       "       -0.62867981, -0.94266809, -0.52432968,  0.72035351,  0.81189733,\n",
       "       -0.93948225,  0.15016645, -0.69112949, -0.82452054, -0.38471749])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reshaped = np.reshape(noise[0], (10,10)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19351976,  0.43048802,  0.21538106, -0.70670491, -0.24176969,\n",
       "        -0.17041408,  0.21306492,  0.52012825,  0.8219043 , -0.05543309],\n",
       "       [ 0.77944314,  0.2339292 , -0.75928628,  0.14310142,  0.79867524,\n",
       "         0.57295251, -0.67927623,  0.46017954,  0.2754592 , -0.62788469],\n",
       "       [-0.99674678, -0.76762486, -0.04542075,  0.03441053, -0.57196575,\n",
       "        -0.84906942,  0.97426391, -0.76840276, -0.90872639, -0.9062041 ],\n",
       "       [ 0.33918074, -0.95678431, -0.05653281, -0.68545544, -0.06655166,\n",
       "        -0.49247393,  0.95511937,  0.62721711, -0.44092348, -0.28514621],\n",
       "       [-0.82510984,  0.02276676,  0.82041728, -0.49635845, -0.18785478,\n",
       "         0.7982896 ,  0.41772887, -0.55441147,  0.90434736,  0.74691212],\n",
       "       [-0.48451421, -0.67880934,  0.70755649, -0.83981037,  0.74736398,\n",
       "         0.74071079,  0.86963838,  0.38207865,  0.97340059,  0.28763157],\n",
       "       [-0.71119976, -0.78375381,  0.18394716,  0.52223724,  0.58767301,\n",
       "        -0.90519094,  0.22012091, -0.60015869,  0.70458812, -0.28245467],\n",
       "       [-0.68811566,  0.40691677,  0.56695819,  0.34074849,  0.63630873,\n",
       "        -0.15277363, -0.23124373,  0.94235075,  0.77956915, -0.37994617],\n",
       "       [ 0.17273848, -0.35332185,  0.75585705,  0.07246529,  0.79613644,\n",
       "         0.56824493, -0.6767984 , -0.17596091, -0.25593972, -0.0166987 ],\n",
       "       [-0.62867981, -0.94266808, -0.52432966,  0.72035348,  0.81189734,\n",
       "        -0.93948227,  0.15016645, -0.69112951, -0.82452053, -0.38471749]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(reshaped.shape[0]):\n",
    "    for t in range(reshaped.shape[1]):\n",
    "        if reshaped[i][t] < 0:\n",
    "            reshaped[i][t] = 0\n",
    "        else:\n",
    "            reshaped[i][t] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19351976,  0.43048802,  0.21538106, -0.70670491, -0.24176969,\n",
       "        -0.17041408,  0.21306492,  0.52012825,  0.8219043 , -0.05543309],\n",
       "       [ 0.77944314,  0.2339292 , -0.75928628,  0.14310142,  0.79867524,\n",
       "         0.57295251, -0.67927623,  0.46017954,  0.2754592 , -0.62788469],\n",
       "       [-0.99674678, -0.76762486, -0.04542075,  0.03441053, -0.57196575,\n",
       "        -0.84906942,  0.97426391, -0.76840276, -0.90872639, -0.9062041 ],\n",
       "       [ 0.33918074, -0.95678431, -0.05653281, -0.68545544, -0.06655166,\n",
       "        -0.49247393,  0.95511937,  0.62721711, -0.44092348, -0.28514621],\n",
       "       [-0.82510984,  0.02276676,  0.82041728, -0.49635845, -0.18785478,\n",
       "         0.7982896 ,  0.41772887, -0.55441147,  0.90434736,  0.74691212],\n",
       "       [-0.48451421, -0.67880934,  0.70755649, -0.83981037,  0.74736398,\n",
       "         0.74071079,  0.86963838,  0.38207865,  0.97340059,  0.28763157],\n",
       "       [-0.71119976, -0.78375381,  0.18394716,  0.52223724,  0.58767301,\n",
       "        -0.90519094,  0.22012091, -0.60015869,  0.70458812, -0.28245467],\n",
       "       [-0.68811566,  0.40691677,  0.56695819,  0.34074849,  0.63630873,\n",
       "        -0.15277363, -0.23124373,  0.94235075,  0.77956915, -0.37994617],\n",
       "       [ 0.17273848, -0.35332185,  0.75585705,  0.07246529,  0.79613644,\n",
       "         0.56824493, -0.6767984 , -0.17596091, -0.25593972, -0.0166987 ],\n",
       "       [-0.62867981, -0.94266808, -0.52432966,  0.72035348,  0.81189734,\n",
       "        -0.93948227,  0.15016645, -0.69112951, -0.82452053, -0.38471749]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x189701cf518>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRE4kBiBA4kROJAYgQOJETiQ2P8Bws+8R+rf57UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1897018ba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([242, 164, 128, 124, 389, 324,  91,  83, 388,  63, 157, 144, 232,\n",
       "        87, 100,  66, 242, 119,  13,  82, 230, 169, 254, 253, 155, 240,\n",
       "       308, 214, 193, 160, 206, 355, 392, 293, 346, 385, 249, 291, 165,\n",
       "       211, 308,  32,  25, 132, 120, 186,  92,  11, 372, 193, 157, 211,\n",
       "       315, 281,  29,  82, 307, 334,  90, 170,  61,  59,  52, 104, 374,\n",
       "        94, 363, 115,  96, 315, 320, 274,  28, 327,  72, 306,  14, 234,\n",
       "        80, 179, 171, 270, 368,  19, 351, 232, 130, 272, 281,  58,   1,\n",
       "        67, 397, 378, 217,  90, 151, 159, 303,  28, 246, 346,  77, 175,\n",
       "       239,  15,  35, 187, 192, 319, 138, 226,  14, 213, 285,  17, 166,\n",
       "        72, 218, 361, 227,  74, 341, 106,  37, 297,  27,  69, 287, 241,\n",
       "        25, 308, 144, 151, 107,  49, 132, 349, 355, 109, 202, 112, 285,\n",
       "       163,  72,  38, 240, 207, 366, 278, 306, 122, 248, 354,  60,  83,\n",
       "        32, 235,  98, 312,  16, 284, 124, 367, 271, 206,  26, 389,  70,\n",
       "       330, 198, 232, 301,  63, 158, 174, 340, 122,  49, 294, 178, 401,\n",
       "       335, 396, 290, 210,  31, 179, 246, 340,   0, 273, 126, 322, 376,\n",
       "       151, 291, 282, 143, 118, 128, 253, 132, 282, 325,  60,  48, 163,\n",
       "       384, 322,  39,  20, 212, 126,  29,  30, 350, 395, 146, 334, 240,\n",
       "       285, 177, 138,  44, 250,  38, 206, 156, 339, 118,   0, 116, 314,\n",
       "        68, 170, 205, 392,   9, 302, 142,  64, 364, 214, 108, 386, 129,\n",
       "        31, 362, 289, 123, 325, 372, 238, 122, 201])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.randint(0, 403, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
