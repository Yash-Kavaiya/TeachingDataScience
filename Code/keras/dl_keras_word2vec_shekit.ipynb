{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abhishek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhishek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk tokenizers\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load books\n",
    "\n",
    "book_filenames = sorted(glob.glob(\"data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/got1.txt',\n",
       " 'data/got2.txt',\n",
       " 'data/got3.txt',\n",
       " 'data/got4.txt',\n",
       " 'data/got5.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length 1770659\n",
      "Corpus length 4071041\n",
      "Corpus length 6391405\n",
      "Corpus length 8107945\n",
      "Corpus length 9719485\n"
     ]
    }
   ],
   "source": [
    "# combine them into single large corpus\n",
    "\n",
    "corpus = u\"\"\n",
    "\n",
    "for book in book_filenames:\n",
    "    with codecs.open(book,\"r\",\"utf-8\") as book_file:\n",
    "        corpus += book_file.read()\n",
    "    print(\"Corpus length {0}\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into sentences\n",
    "\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split using tokenizer\n",
    "\n",
    "raw_sentences = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was here the ravens came, after long flight.\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non letters and split into words\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split raw sentences\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for raw in raw_sentences:\n",
    "    if len(raw)>0:\n",
    "        sentences.append(sentence_to_wordlist(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'here', 'the', 'ravens', 'came', 'after', 'long', 'flight']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1818103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word 2 vec\n",
    "\n",
    "word2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=1,\n",
    "    workers= multiprocessing.cpu_count(),\n",
    "    size=300,\n",
    "    min_count=3,\n",
    "    window=7,\n",
    "    sample=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocab\n",
    "\n",
    "word2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17277"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7021134"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "word2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jorah', 0.5570077896118164),\n",
       " ('Doreah', 0.5511056184768677),\n",
       " ('Khal', 0.5330901145935059),\n",
       " ('Kraznys', 0.5310728549957275),\n",
       " ('handmaids', 0.527921736240387),\n",
       " ('Drogo', 0.5277822017669678),\n",
       " ('Jhogo', 0.5204181671142578),\n",
       " ('Rakharo', 0.5188958644866943),\n",
       " ('Naharis', 0.5162404775619507),\n",
       " ('Duur', 0.5159556269645691)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(\"Dany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.8006671071052551),\n",
       " ('queen', 0.7967936396598816),\n",
       " ('Dalla', 0.7949143648147583),\n",
       " ('seamstress', 0.7635628581047058),\n",
       " ('Alayaya', 0.7592761516571045),\n",
       " ('kneeling', 0.7547597289085388),\n",
       " ('pregnant', 0.7514938116073608),\n",
       " ('goddess', 0.7475717067718506),\n",
       " ('shares', 0.7472423911094666),\n",
       " ('womanhood', 0.7460522055625916)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar_cosmul(positive=[\"king\",\"woman\"],negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
