{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Named Entity Recognition (NER) using Tensorflow\n",
    "\n",
    "(Original: Named Entity Recognition (NER) for CoNLL dataset with Tensorflow 2.2.0 [https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a]  by Bhuvana Kundumani)\n",
    "Refernce: https://github.com/bhuvanakundumani/NER_tensorflow2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) identifes named entities like ‘America’ , ‘Emily’ , ‘London’ ,etc.. and categorize them as PERSON, LOCATION , and so on. In spacy, NER is implemented by the pipeline component 'ner'. This notebook details the steps for Named Entity Recognition (NER) tagging of sentences (CoNLL-2003 dataset ) using Tensorflow2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data source: https://www.clips.uantwerpen.be/conll2003/ner/\n",
    "\n",
    "CoNLL-2003 dataset includes 1,393 English and 909 German news articles. Only English here. Data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence.\n",
    "\n",
    "![CoNLL Data](images/ner1.png)\n",
    "\n",
    "Need only the named entity tags. We extract the words along with their named entities into an array — [ [‘EU’, ‘B-ORG’], [‘rejects’, ‘O’], [‘German’, ‘B-MISC’], [‘call’, ‘O’], [‘to’, ‘O’], [‘boycott’, ‘O’], [‘British’, ‘B-MISC’], [‘lamb’, ‘O’], [‘.’, ‘O’] ]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_label(filename):\n",
    "  f = open(filename)\n",
    "  split_labeled_text = []\n",
    "  sentence = []\n",
    "  for line in f:\n",
    "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "       if len(sentence) > 0:\n",
    "         split_labeled_text.append(sentence)\n",
    "         sentence = []\n",
    "       continue\n",
    "    splits = line.split(' ')\n",
    "    sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
    "  if len(sentence) > 0:\n",
    "    split_labeled_text.append(sentence)\n",
    "    sentence = []\n",
    "  return split_labeled_text\n",
    "split_train = split_text_label(\"data/conll_ner/train.txt\")\n",
    "split_valid = split_text_label(\"data/conll_ner/valid.txt\")\n",
    "split_test = split_text_label(\"data/conll_ner/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary for all unique words and unique labels (named entities will be referred to as labels) in the folders (train, valid and test). labelSet contains all the unique words in the labels i.e the named entities. wordSet contains all the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "wordSet = set()\n",
    "# words and labels\n",
    "for data in [split_train, split_valid, split_test]:\n",
    "  for labeled_text in data:\n",
    "    for word, label in labeled_text:\n",
    "      labelSet.add(label)\n",
    "      wordSet.add(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate a unique index to each word/ label in the vocabulary. We assign index 0 for ‘PADDING_TOKEN’ and 1 for ‘UNKNOWN_TOKEN’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the set to ensure '0' is assigned to 0\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "# Create mapping for labels\n",
    "label2Idx = {}\n",
    "for label in sorted_labels:\n",
    "  label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "# Create mapping for words\n",
    "word2Idx = {}\n",
    "if len(word2Idx) == 0:\n",
    "  word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "  word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "for word in wordSet:\n",
    "  word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the words in the split_train, split_valid and split_test folders and convert the words and labels in them to their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(data, word2Idx, label2Idx):\n",
    "  sentences = []\n",
    "  labels = []\n",
    "  for split_labeled_text in data:\n",
    "     wordIndices = []\n",
    "     labelIndices = []\n",
    "     for word, label in split_labeled_text:\n",
    "       if word in word2Idx:\n",
    "          wordIdx = word2Idx[word]\n",
    "       elif word.lower() in word2Idx:\n",
    "          wordIdx = word2Idx[word.lower()]\n",
    "       else:\n",
    "          wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "       wordIndices.append(wordIdx)\n",
    "       labelIndices.append(label2Idx[label])\n",
    "     sentences.append(wordIndices)\n",
    "     labels.append(labelIndices)\n",
    "  return sentences, labels\n",
    "train_sentences, train_labels = createMatrices(split_train, word2Idx, label2Idx)\n",
    "valid_sentences, valid_labels = createMatrices(split_valid, word2Idx, label2Idx)\n",
    "test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences are of different lengths. We need to pad the sentences and the labels in order to make them of equal lengths. max_seq_len is taken as 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sentences, labels, max_len, padding='post'):\n",
    "  padded_sentences = pad_sequences(sentences, max_len,       \n",
    "  padding='post')\n",
    "  padded_labels = pad_sequences(labels, max_len, padding='post')\n",
    "  return padded_sentences, padded_labels\n",
    "\n",
    "# padding sentences and labels to max_length of 128\n",
    "max_seq_len = 128\n",
    "\n",
    "    \n",
    "train_features, train_labels = padding(train_sentences, train_labels, max_seq_len, padding='post' )\n",
    "valid_features, valid_labels = padding(valid_sentences, valid_labels, max_seq_len, padding='post' )\n",
    "test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pre-trained Glove word Embeddings. \n",
    "\n",
    "Download the Glove embeddings — glove.6B.100d.txt from http://nlp.stanford.edu/data/glove.6B.zip to the embeddings folder. \n",
    "\n",
    "For all the words in our vocabulary, we get the Glove representation for the words. embedding_vector has the Glove representation for all the words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-7a2f085cc72c>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-7a2f085cc72c>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    representing the word\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Loading glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt', encoding=\"utf-8\")\n",
    "for line in f:\n",
    "  values = line.strip().split(' ')\n",
    "  word = values[0] # the first entry is the word\n",
    "  coefs = np.asarray(values[1:], dtype='float32') #100d vectors   \n",
    "  representing the word\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embedding_matrix = np.zeros((len(word2Idx), EMBEDDING_DIM))\n",
    "# Word embeddings for the tokens\n",
    "for word,i in word2Idx.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tf.data.Dataset.from_tensor_slices for batching and shuffling the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "valid_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_features, valid_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "shuffled_train_dataset = train_dataset.shuffle(buffer_size=train_features.shape[0], reshuffle_each_iteration=True)\n",
    "\n",
    "batched_train_dataset = shuffled_train_dataset.batch(train_batch_size, drop_remainder=True)\n",
    "batched_valid_dataset = valid_dataset.batch(valid_batch_size, drop_remainder=True)\n",
    "batched_test_dataset = test_dataset.batch(test_batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "class TFNer(tf.keras.Model):\n",
    "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, num_labels, weights):\n",
    "       super(TFNer, self).__init__() \n",
    "       self.embedding = layers.Embedding(input_dim=embed_input_dim, \n",
    "       output_dim=embed_output_dim, weights=weights,    \n",
    "       input_length=max_seq_len, trainable=False, mask_zero=True)        \n",
    "\n",
    "       self.bilstm = layers.Bidirectional(layers.LSTM(128,  \n",
    "       return_sequences=True))\n",
    "       self.dense = layers.Dense(num_labels)\n",
    "    def call(self, inputs):\n",
    "       x = self.embedding(inputs) # batchsize, max_seq_len,      \n",
    "       embedding_output_dim\n",
    "       x = self.bilstm(x) #batchsize, max_seq_len, hidden_dim_bilstm\n",
    "       logits = self.dense(x) #batchsize, max_seq_len, num_labels\n",
    "       return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a bidirectional LSTM after the embedding and we have a fully connected layer that transforms the output of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-57e002c7a427>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFNer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_input_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2Idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_output_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "model = TFNer(max_seq_len=max_seq_len,embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \\content\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "valid_loss_metric = tf.keras.metrics.Mean('valid_loss', dtype=tf.float32)\n",
    "\n",
    "epoch_bar = 10\n",
    "\n",
    "    \n",
    "def train_step_fn(sentences_batch, labels_batch):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = model(sentences_batch)\n",
    "    loss = scce(labels_batch, logits)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(list(zip(grads,   \n",
    "  model.trainable_variables)))\n",
    "  return loss, logits\n",
    "\n",
    "def valid_step_fn(sentences_batch, labels_batch):\n",
    "  logits = model(sentences_batch)\n",
    "  loss = scce(labels_batch, logits)\n",
    "  return loss, logits\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "  for sentences_batch, labels_batch in progress_bar(batched_train_dataset, total=train_pb_max_len, parent=epoch_bar) :\n",
    "    loss, logits = train_step_fn(sentences_batch, labels_batch)\n",
    "    train_loss_metric(loss)\n",
    "  train_loss_metric.reset_states()\n",
    "  for sentences_batch, labels_batch in \n",
    "  progress_bar(batched_valid_dataset, total=valid_pb_max_len, \n",
    "  parent=epoch_bar):\n",
    "    loss, logits = valid_step_fn(sentences_batch, labels_batch\n",
    "    valid_loss_metric.update_state(loss)\n",
    "  valid_loss_metric.reset_states()\n",
    "                                 \n",
    "model.save_weights(f\"{args.output}/model_weights\",save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance on test dataset\n",
    "\n",
    "Use precision, recall and f1 score for evaluating the model. We use seqeval package. seqeval is a Python framework for sequence labelling evaluation. seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labelling and so on. classification_report metric builds a text report showing the main classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_label(predictions, correct, idx2Label):\n",
    "  label_pred = []\n",
    "  for sentence in predictions:\n",
    "    for i in sentence:\n",
    "      label_pred.append([idx2Label[elem] for elem in i ])\n",
    "  label_correct = []\n",
    "  if correct != None:\n",
    "    for sentence in correct:\n",
    "    for i in sentence:\n",
    "      label_correct.append([idx2Label[elem] for elem in i ])\n",
    "  return label_correct, label_pred\n",
    "\n",
    "test_model =  TFNer(max_seq_len=max_seq_len, embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "test_model.load_weights(f\"{args.output}/model_weights\")\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for sentences_batch, labels_batch in progress_bar(batched_test_dataset, total=test_pb_max_len):\n",
    "  logits = test_model(sentences_batch)\n",
    "  temp1 = tf.nn.softmax(logits)\n",
    "  preds = tf.argmax(temp1, axis=2)\n",
    "  true_labels.append(np.asarray(labels_batch))\n",
    "  pred_labels.append(np.asarray(preds))\n",
    "\n",
    "label_correct, label_pred = idx_to_label(pred_labels, true_labels, idx2Label)\n",
    "\n",
    "report = classification_report(label_correct, label_pred, digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass sufficient examples and good number of iterations, say 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 6.135417460280541}\n",
      "Losses {'ner': 18.055886029082423}\n",
      "Losses {'ner': 26.98418468125451}\n",
      "Losses {'ner': 29.93112833611222}\n",
      "Losses {'ner': 42.66409005037997}\n",
      "Losses {'ner': 47.60032568394229}\n",
      "Losses {'ner': 52.556862453945804}\n",
      "Losses {'ner': 57.99250314309007}\n",
      "Losses {'ner': 63.61409512900268}\n",
      "Losses {'ner': 65.15030186584116}\n",
      "Losses {'ner': 72.25433349458224}\n",
      "Losses {'ner': 77.04438918883169}\n",
      "Losses {'ner': 83.47645027281031}\n",
      "Losses {'ner': 90.64281290115103}\n",
      "Losses {'ner': 1.9889164222981925}\n",
      "Losses {'ner': 6.92639906005658}\n",
      "Losses {'ner': 9.8967614361081}\n",
      "Losses {'ner': 14.314272661293769}\n",
      "Losses {'ner': 16.30601631543266}\n",
      "Losses {'ner': 19.142050187208387}\n",
      "Losses {'ner': 27.31261651880357}\n",
      "Losses {'ner': 34.23241859108691}\n",
      "Losses {'ner': 44.25706630379443}\n",
      "Losses {'ner': 49.1747580840816}\n",
      "Losses {'ner': 53.14143339348539}\n",
      "Losses {'ner': 61.481861582634345}\n",
      "Losses {'ner': 66.44031007089961}\n",
      "Losses {'ner': 73.32213563857846}\n",
      "Losses {'ner': 3.7872812948189676}\n",
      "Losses {'ner': 7.356638347862727}\n",
      "Losses {'ner': 14.315541165082223}\n",
      "Losses {'ner': 21.27128299734386}\n",
      "Losses {'ner': 26.360053868669297}\n",
      "Losses {'ner': 32.749179282861974}\n",
      "Losses {'ner': 35.3576765727134}\n",
      "Losses {'ner': 39.40600076168762}\n",
      "Losses {'ner': 48.36320856204243}\n",
      "Losses {'ner': 49.98927343167078}\n",
      "Losses {'ner': 54.30223295111465}\n",
      "Losses {'ner': 62.27424779553222}\n",
      "Losses {'ner': 71.12656784822826}\n",
      "Losses {'ner': 74.64730171506886}\n",
      "Losses {'ner': 3.837474623827802}\n",
      "Losses {'ner': 7.023237131077622}\n",
      "Losses {'ner': 11.226175330120896}\n",
      "Losses {'ner': 13.824300641856098}\n",
      "Losses {'ner': 17.950286396247975}\n",
      "Losses {'ner': 23.814348201696703}\n",
      "Losses {'ner': 28.819193520706904}\n",
      "Losses {'ner': 34.3554601743308}\n",
      "Losses {'ner': 39.058779829858395}\n",
      "Losses {'ner': 41.11634054348542}\n",
      "Losses {'ner': 49.055249913006264}\n",
      "Losses {'ner': 55.46605990675016}\n",
      "Losses {'ner': 61.759759206201124}\n",
      "Losses {'ner': 62.93088056646957}\n",
      "Losses {'ner': 7.46548193693161}\n",
      "Losses {'ner': 12.475946867838502}\n",
      "Losses {'ner': 14.715780345839448}\n",
      "Losses {'ner': 19.379613357712515}\n",
      "Losses {'ner': 21.492392465952435}\n",
      "Losses {'ner': 26.427014586995938}\n",
      "Losses {'ner': 28.474785509257345}\n",
      "Losses {'ner': 30.459554492346797}\n",
      "Losses {'ner': 33.56917686007819}\n",
      "Losses {'ner': 38.90160333357653}\n",
      "Losses {'ner': 42.15796841586871}\n",
      "Losses {'ner': 47.546773313534686}\n",
      "Losses {'ner': 50.51715795601649}\n",
      "Losses {'ner': 55.97416686172528}\n",
      "Losses {'ner': 3.5426370768836932}\n",
      "Losses {'ner': 9.035783693849226}\n",
      "Losses {'ner': 16.996129558145185}\n",
      "Losses {'ner': 17.399982804592582}\n",
      "Losses {'ner': 21.31895661719318}\n",
      "Losses {'ner': 24.02514165789762}\n",
      "Losses {'ner': 29.16991982669788}\n",
      "Losses {'ner': 34.24550335611639}\n",
      "Losses {'ner': 36.72093247752127}\n",
      "Losses {'ner': 39.750699689047906}\n",
      "Losses {'ner': 42.839219865756604}\n",
      "Losses {'ner': 42.84561978149213}\n",
      "Losses {'ner': 49.25083923790589}\n",
      "Losses {'ner': 55.31549952234127}\n",
      "Losses {'ner': 2.7445076965860835}\n",
      "Losses {'ner': 7.330554314471669}\n",
      "Losses {'ner': 13.405413314282782}\n",
      "Losses {'ner': 18.246378570843376}\n",
      "Losses {'ner': 21.00167666262962}\n",
      "Losses {'ner': 23.09337816906293}\n",
      "Losses {'ner': 27.616286793044424}\n",
      "Losses {'ner': 32.706600760361766}\n",
      "Losses {'ner': 36.15724442106614}\n",
      "Losses {'ner': 38.87151251939633}\n",
      "Losses {'ner': 42.33842188066103}\n",
      "Losses {'ner': 46.070172033898984}\n",
      "Losses {'ner': 50.59298455847659}\n",
      "Losses {'ner': 56.16666128805139}\n",
      "Losses {'ner': 2.9328090430267366}\n",
      "Losses {'ner': 5.053547227048057}\n",
      "Losses {'ner': 8.838060450149442}\n",
      "Losses {'ner': 11.886917634276415}\n",
      "Losses {'ner': 16.871712757615114}\n",
      "Losses {'ner': 23.594727976239028}\n",
      "Losses {'ner': 26.493683334431353}\n",
      "Losses {'ner': 29.551258087382394}\n",
      "Losses {'ner': 34.17946633486383}\n",
      "Losses {'ner': 36.4293552119135}\n",
      "Losses {'ner': 39.631460658632136}\n",
      "Losses {'ner': 39.74428134447328}\n",
      "Losses {'ner': 42.84927193367287}\n",
      "Losses {'ner': 49.812091260341845}\n",
      "Losses {'ner': 2.5712037570774555}\n",
      "Losses {'ner': 7.240999836474657}\n",
      "Losses {'ner': 15.390614587813616}\n",
      "Losses {'ner': 20.465302353724837}\n",
      "Losses {'ner': 20.473544779481017}\n",
      "Losses {'ner': 24.536050009788596}\n",
      "Losses {'ner': 27.289035161040374}\n",
      "Losses {'ner': 34.2902789311629}\n",
      "Losses {'ner': 39.09018169682531}\n",
      "Losses {'ner': 44.270017326596644}\n",
      "Losses {'ner': 45.78610100840888}\n",
      "Losses {'ner': 50.56419438553712}\n",
      "Losses {'ner': 53.51935869105364}\n",
      "Losses {'ner': 59.430226445423614}\n",
      "Losses {'ner': 4.920932404696941}\n",
      "Losses {'ner': 8.542504986864515}\n",
      "Losses {'ner': 14.456327928346582}\n",
      "Losses {'ner': 17.46570686076302}\n",
      "Losses {'ner': 24.87515497591812}\n",
      "Losses {'ner': 32.352780822780915}\n",
      "Losses {'ner': 34.38721761253055}\n",
      "Losses {'ner': 40.20395396310141}\n",
      "Losses {'ner': 47.179153245270754}\n",
      "Losses {'ner': 51.66110027826744}\n",
      "Losses {'ner': 54.127964653665}\n",
      "Losses {'ner': 57.550144978690696}\n",
      "Losses {'ner': 60.343656772624854}\n",
      "Losses {'ner': 62.2820460905059}\n",
      "Losses {'ner': 5.455496720969677}\n",
      "Losses {'ner': 10.497991466138046}\n",
      "Losses {'ner': 13.123656198673416}\n",
      "Losses {'ner': 17.346978162939195}\n",
      "Losses {'ner': 19.339893866505008}\n",
      "Losses {'ner': 22.91734290047316}\n",
      "Losses {'ner': 25.139569650113117}\n",
      "Losses {'ner': 25.209752326754824}\n",
      "Losses {'ner': 29.279065558144794}\n",
      "Losses {'ner': 32.157509053149624}\n",
      "Losses {'ner': 33.23732714206744}\n",
      "Losses {'ner': 34.19738833464521}\n",
      "Losses {'ner': 37.66957497268754}\n",
      "Losses {'ner': 43.9930657711609}\n",
      "Losses {'ner': 3.696151375770569}\n",
      "Losses {'ner': 7.359835646115243}\n",
      "Losses {'ner': 20.774117968045175}\n",
      "Losses {'ner': 24.637702441781585}\n",
      "Losses {'ner': 27.38252679231664}\n",
      "Losses {'ner': 31.906750668735185}\n",
      "Losses {'ner': 33.575054299981275}\n",
      "Losses {'ner': 34.548712946486376}\n",
      "Losses {'ner': 41.24414065562905}\n",
      "Losses {'ner': 44.482561948590046}\n",
      "Losses {'ner': 50.49074356632349}\n",
      "Losses {'ner': 51.9115334451389}\n",
      "Losses {'ner': 55.654482902127896}\n",
      "Losses {'ner': 60.40832734929104}\n",
      "Losses {'ner': 0.9742045403940978}\n",
      "Losses {'ner': 1.9614772436357129}\n",
      "Losses {'ner': 7.048964823712353}\n",
      "Losses {'ner': 8.014656544711485}\n",
      "Losses {'ner': 10.921123952653303}\n",
      "Losses {'ner': 14.874670269947543}\n",
      "Losses {'ner': 19.693061666378796}\n",
      "Losses {'ner': 25.3382522097575}\n",
      "Losses {'ner': 29.93680348054322}\n",
      "Losses {'ner': 37.06429249600323}\n",
      "Losses {'ner': 46.10848124504837}\n",
      "Losses {'ner': 48.93466638580712}\n",
      "Losses {'ner': 51.800794106346345}\n",
      "Losses {'ner': 53.743466987262124}\n",
      "Losses {'ner': 2.1906465537172153}\n",
      "Losses {'ner': 4.359611297864376}\n",
      "Losses {'ner': 10.775301229311935}\n",
      "Losses {'ner': 13.591181434008377}\n",
      "Losses {'ner': 21.438859815841624}\n",
      "Losses {'ner': 27.045875410681674}\n",
      "Losses {'ner': 35.27910725319953}\n",
      "Losses {'ner': 37.67946305142823}\n",
      "Losses {'ner': 39.788988284775314}\n",
      "Losses {'ner': 45.87458170675882}\n",
      "Losses {'ner': 49.02908656471618}\n",
      "Losses {'ner': 51.36130121574888}\n",
      "Losses {'ner': 59.5373795334358}\n",
      "Losses {'ner': 62.656751429429505}\n",
      "Losses {'ner': 4.480079650878906}\n",
      "Losses {'ner': 6.37264536076691}\n",
      "Losses {'ner': 10.177614908549003}\n",
      "Losses {'ner': 15.45349055621773}\n",
      "Losses {'ner': 23.41808891389519}\n",
      "Losses {'ner': 23.441493196787633}\n",
      "Losses {'ner': 30.149767049730144}\n",
      "Losses {'ner': 34.876536450490676}\n",
      "Losses {'ner': 39.34420792399396}\n",
      "Losses {'ner': 39.35628272233953}\n",
      "Losses {'ner': 44.346685338979626}\n",
      "Losses {'ner': 49.41835282437711}\n",
      "Losses {'ner': 53.64810743794584}\n",
      "Losses {'ner': 55.60244298434014}\n",
      "Losses {'ner': 4.773640334839001}\n",
      "Losses {'ner': 10.685256627737544}\n",
      "Losses {'ner': 11.759042913559824}\n",
      "Losses {'ner': 17.30586988838877}\n",
      "Losses {'ner': 20.111122741766394}\n",
      "Losses {'ner': 25.24855137610075}\n",
      "Losses {'ner': 26.961471087992777}\n",
      "Losses {'ner': 29.297379016754803}\n",
      "Losses {'ner': 30.296863129857343}\n",
      "Losses {'ner': 36.94972693201282}\n",
      "Losses {'ner': 39.42297432877251}\n",
      "Losses {'ner': 44.88557408776262}\n",
      "Losses {'ner': 52.41543088879564}\n",
      "Losses {'ner': 57.189453988830905}\n",
      "Losses {'ner': 3.2178475006949157}\n",
      "Losses {'ner': 8.911354783620709}\n",
      "Losses {'ner': 18.323153135385155}\n",
      "Losses {'ner': 20.00538981239174}\n",
      "Losses {'ner': 20.987341018036886}\n",
      "Losses {'ner': 25.31572345159293}\n",
      "Losses {'ner': 28.497666874940478}\n",
      "Losses {'ner': 32.06689584608466}\n",
      "Losses {'ner': 35.51888090236389}\n",
      "Losses {'ner': 37.44723029505258}\n",
      "Losses {'ner': 37.448401681294854}\n",
      "Losses {'ner': 41.40345869833436}\n",
      "Losses {'ner': 47.51840774589982}\n",
      "Losses {'ner': 52.73112292928454}\n",
      "Losses {'ner': 5.521322200693248}\n",
      "Losses {'ner': 6.769024614978434}\n",
      "Losses {'ner': 9.299482280698044}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 15.070842868026716}\n",
      "Losses {'ner': 17.41177810350598}\n",
      "Losses {'ner': 20.929445350954097}\n",
      "Losses {'ner': 25.880433935083452}\n",
      "Losses {'ner': 32.75067881063372}\n",
      "Losses {'ner': 37.85085251409017}\n",
      "Losses {'ner': 40.69515348538933}\n",
      "Losses {'ner': 46.009373735042196}\n",
      "Losses {'ner': 53.27040446996633}\n",
      "Losses {'ner': 57.52870198565014}\n",
      "Losses {'ner': 65.76614764546557}\n",
      "Losses {'ner': 4.269399568118388}\n",
      "Losses {'ner': 9.889373769663507}\n",
      "Losses {'ner': 13.241571735736215}\n",
      "Losses {'ner': 15.619898533834203}\n",
      "Losses {'ner': 24.24267604007764}\n",
      "Losses {'ner': 24.254182549579127}\n",
      "Losses {'ner': 28.367168055854563}\n",
      "Losses {'ner': 31.062403062631347}\n",
      "Losses {'ner': 36.04521516179011}\n",
      "Losses {'ner': 37.667203489265376}\n",
      "Losses {'ner': 40.24817592875843}\n",
      "Losses {'ner': 47.3054819800127}\n",
      "Losses {'ner': 50.601195993291185}\n",
      "Losses {'ner': 53.497107728657284}\n",
      "Losses {'ner': 3.7994599402516087}\n",
      "Losses {'ner': 7.913474875472957}\n",
      "Losses {'ner': 8.871293293435656}\n",
      "Losses {'ner': 13.129610070431}\n",
      "Losses {'ner': 18.005597121818255}\n",
      "Losses {'ner': 21.45686332244918}\n",
      "Losses {'ner': 23.4375586244671}\n",
      "Losses {'ner': 31.926307210912682}\n",
      "Losses {'ner': 37.89031499273119}\n",
      "Losses {'ner': 42.03998352406256}\n",
      "Losses {'ner': 43.98633588726284}\n",
      "Losses {'ner': 50.42822582895519}\n",
      "Losses {'ner': 53.84277981471837}\n",
      "Losses {'ner': 56.01318739414677}\n",
      "Losses {'ner': 0.23327851109661424}\n",
      "Losses {'ner': 2.349779745307842}\n",
      "Losses {'ner': 5.1872203110019655}\n",
      "Losses {'ner': 8.204827486281374}\n",
      "Losses {'ner': 10.281333395949218}\n",
      "Losses {'ner': 15.339612284413192}\n",
      "Losses {'ner': 19.17351095654874}\n",
      "Losses {'ner': 23.802547120577287}\n",
      "Losses {'ner': 25.94348077863185}\n",
      "Losses {'ner': 30.528303948377356}\n",
      "Losses {'ner': 37.58676575828656}\n",
      "Losses {'ner': 41.44353610625956}\n",
      "Losses {'ner': 42.80034679566859}\n",
      "Losses {'ner': 46.02670013503928}\n",
      "Losses {'ner': 7.4740142822265625}\n",
      "Losses {'ner': 8.700487183210498}\n",
      "Losses {'ner': 12.707028264107574}\n",
      "Losses {'ner': 18.540835578335244}\n",
      "Losses {'ner': 26.068862248552758}\n",
      "Losses {'ner': 30.053955034934287}\n",
      "Losses {'ner': 30.053968669621845}\n",
      "Losses {'ner': 34.19224637525878}\n",
      "Losses {'ner': 42.37966403260618}\n",
      "Losses {'ner': 42.397540021604755}\n",
      "Losses {'ner': 46.53808376091615}\n",
      "Losses {'ner': 49.9353200292272}\n",
      "Losses {'ner': 49.935744940161925}\n",
      "Losses {'ner': 53.38895897783249}\n",
      "Losses {'ner': 2.8930259824938958}\n",
      "Losses {'ner': 12.345428454989928}\n",
      "Losses {'ner': 14.289150488478185}\n",
      "Losses {'ner': 17.775981536064933}\n",
      "Losses {'ner': 20.332564982198733}\n",
      "Losses {'ner': 27.686597979107887}\n",
      "Losses {'ner': 27.688507951669862}\n",
      "Losses {'ner': 31.856093887690918}\n",
      "Losses {'ner': 37.176388987791256}\n",
      "Losses {'ner': 44.33779805128154}\n",
      "Losses {'ner': 46.29656071885995}\n",
      "Losses {'ner': 53.8710554375881}\n",
      "Losses {'ner': 56.3620819115082}\n",
      "Losses {'ner': 58.62783512434055}\n",
      "Losses {'ner': 0.00020967370136304453}\n",
      "Losses {'ner': 5.289966165481644}\n",
      "Losses {'ner': 7.918031957019309}\n",
      "Losses {'ner': 7.939904869885652}\n",
      "Losses {'ner': 15.224150657018583}\n",
      "Losses {'ner': 20.904468804871527}\n",
      "Losses {'ner': 22.8424551142206}\n",
      "Losses {'ner': 26.393602089374454}\n",
      "Losses {'ner': 32.3221462654184}\n",
      "Losses {'ner': 34.84873721388534}\n",
      "Losses {'ner': 38.83597847083575}\n",
      "Losses {'ner': 40.92930668717718}\n",
      "Losses {'ner': 46.371311172232986}\n",
      "Losses {'ner': 52.24103786007736}\n",
      "Losses {'ner': 5.106609736827522}\n",
      "Losses {'ner': 10.808427525011211}\n",
      "Losses {'ner': 12.068395693551722}\n",
      "Losses {'ner': 20.485571353898123}\n",
      "Losses {'ner': 25.068299683740925}\n",
      "Losses {'ner': 30.96389392297585}\n",
      "Losses {'ner': 35.201537401996184}\n",
      "Losses {'ner': 37.50743839699732}\n",
      "Losses {'ner': 38.7068460499491}\n",
      "Losses {'ner': 40.03677580715017}\n",
      "Losses {'ner': 40.04653380837632}\n",
      "Losses {'ner': 46.445612710233455}\n",
      "Losses {'ner': 50.08609063970149}\n",
      "Losses {'ner': 52.86305326942462}\n",
      "Losses {'ner': 4.620006332115736}\n",
      "Losses {'ner': 9.968512231469504}\n",
      "Losses {'ner': 15.925855273366324}\n",
      "Losses {'ner': 21.26081553554104}\n",
      "Losses {'ner': 23.250504156399074}\n",
      "Losses {'ner': 28.080221118893228}\n",
      "Losses {'ner': 32.04196164898922}\n",
      "Losses {'ner': 37.76997982111816}\n",
      "Losses {'ner': 40.29361863675995}\n",
      "Losses {'ner': 44.77221663794301}\n",
      "Losses {'ner': 45.33303128198718}\n",
      "Losses {'ner': 50.63966713880179}\n",
      "Losses {'ner': 51.69324991460578}\n",
      "Losses {'ner': 55.28155148026714}\n",
      "Losses {'ner': 4.423350081029467}\n",
      "Losses {'ner': 6.387791406836014}\n",
      "Losses {'ner': 10.577826049651604}\n",
      "Losses {'ner': 15.787484301494088}\n",
      "Losses {'ner': 24.627614690230814}\n",
      "Losses {'ner': 27.0941204264075}\n",
      "Losses {'ner': 30.41237645067875}\n",
      "Losses {'ner': 33.28215374061645}\n",
      "Losses {'ner': 35.44766661255224}\n",
      "Losses {'ner': 35.46793485738783}\n",
      "Losses {'ner': 38.97346341537525}\n",
      "Losses {'ner': 43.28668679653083}\n",
      "Losses {'ner': 46.63416686014125}\n",
      "Losses {'ner': 51.43666601440896}\n",
      "Losses {'ner': 2.921364856825676}\n",
      "Losses {'ner': 8.850782604713459}\n",
      "Losses {'ner': 9.907053090952104}\n",
      "Losses {'ner': 13.739573374710744}\n",
      "Losses {'ner': 16.72475501181907}\n",
      "Losses {'ner': 16.732254043292215}\n",
      "Losses {'ner': 16.737226608026262}\n",
      "Losses {'ner': 19.328335751872814}\n",
      "Losses {'ner': 27.22624430453901}\n",
      "Losses {'ner': 30.764321196277464}\n",
      "Losses {'ner': 33.66024278451253}\n",
      "Losses {'ner': 36.01022666266114}\n",
      "Losses {'ner': 36.01031126929514}\n",
      "Losses {'ner': 42.88367367085369}\n",
      "Losses {'ner': 2.2865281177219003}\n",
      "Losses {'ner': 3.2536276420842114}\n",
      "Losses {'ner': 7.062014278516017}\n",
      "Losses {'ner': 12.216842797145091}\n",
      "Losses {'ner': 15.876738714757039}\n",
      "Losses {'ner': 20.918129797804028}\n",
      "Losses {'ner': 24.060590920827195}\n",
      "Losses {'ner': 26.46218644582146}\n",
      "Losses {'ner': 30.542117006001735}\n",
      "Losses {'ner': 34.3499906722882}\n",
      "Losses {'ner': 35.63791503886297}\n",
      "Losses {'ner': 39.921280553923694}\n",
      "Losses {'ner': 44.03897003522549}\n",
      "Losses {'ner': 47.4103504458849}\n",
      "Losses {'ner': 3.6872000180077205}\n",
      "Losses {'ner': 3.68848816371416}\n",
      "Losses {'ner': 4.665313901502818}\n",
      "Losses {'ner': 10.528799186583788}\n",
      "Losses {'ner': 12.912058345165283}\n",
      "Losses {'ner': 15.083685933007805}\n",
      "Losses {'ner': 17.610977687031998}\n",
      "Losses {'ner': 20.164465436533405}\n",
      "Losses {'ner': 27.434063860968067}\n",
      "Losses {'ner': 30.038055491484315}\n",
      "Losses {'ner': 33.967655120936556}\n",
      "Losses {'ner': 33.968895364764464}\n",
      "Losses {'ner': 38.720690426920484}\n",
      "Losses {'ner': 42.623659206973954}\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "  sizes = compounding(1.0, 4.0, 1.001)\n",
    "  # Training for 30 iterations     \n",
    "  for itn in range(30):\n",
    "    # shuffle examples before training\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "    # ictionary to store losses\n",
    "    losses = {}\n",
    "    for batch in batches:\n",
    "      texts, annotations = zip(*batch)\n",
    "      # Calling update() over the iteration\n",
    "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "      print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to \\content\n",
      "Loading from \\content\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('/content/')\n",
    "\n",
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "  output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \\content\n",
      "No Entities found!!\n"
     ]
    }
   ],
   "source": [
    "# Loading the model from the directory\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "doc2 = nlp2(' Idli is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "  print(ent.label_, ent.text)\n",
    "else:\n",
    "  print(\"No Entities found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
