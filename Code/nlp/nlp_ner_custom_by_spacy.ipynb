{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Named Entity Recognition (NER) using Spacy\n",
    "\n",
    "(Original: How to Train spaCy to Autodetect New Entities (NER)[https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/]  by Shrivarsheni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) identifes named entities like ‘America’ , ‘Emily’ , ‘London’ ,etc.. and categorize them as PERSON, LOCATION , and so on. In spacy, NER is implemented by the pipeline component 'ner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a spacy model and chekc if it has ner\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default NER\n",
    "\n",
    "Testing default 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India GPE\n",
      "Flipkart PERSON\n",
      "2007 DATE\n",
      "Flipkart PERSON\n",
      "Indian NORP\n",
      "Amazon ORG\n",
      "Walmart LOC\n",
      "one CARDINAL\n",
      "US GPE\n",
      "Amazon ORG\n",
      "daily DATE\n",
      "2010 DATE\n",
      "2011 DATE\n",
      "more than 3 CARDINAL\n",
      "India GPE\n",
      "over 30 million CARDINAL\n",
      "over 125,000 CARDINAL\n",
      "Indian NORP\n",
      "recent years DATE\n",
      "Freecharge PERSON\n",
      "Unicommerce GPE\n",
      "One CARDINAL\n",
      "2013 DATE\n",
      "the last 5 years DATE\n",
      "daily DATE\n",
      "India GPE\n",
      "Indian NORP\n"
     ]
    }
   ],
   "source": [
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "\n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes.\"\"\"\n",
    "\n",
    "doc=nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that 'Flipkar' is marked as PERSON instead of ORG. 'Snapdeal' is not even tagged. Need to update the default model.\n",
    "\n",
    "Our task is make sure the NER recognizes the company asORGand not as PERSON , place the unidentified products under PRODUCT and so on.\n",
    "\n",
    "To enable this, you need to provide training examples which will make the NER learn for future samples.\n",
    "\n",
    "To do this, let’s use an existing pre-trained spacy model and update it with newer examples.\n",
    "\n",
    "First , let’s load a pre-existing spacy model with an in-built ner component. Then, get the Named Entity Recognizer using get_pipe() method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-existing spacy model\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update a pretrained model with new examples, need to provide many examples to meaningfully improve the system — a few hundred is a good start, although more is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Prep\n",
    "\n",
    "Format: list of tuples where each tuple should contain the text and a dictionary. The dictionary should hold the start and end indices of the named enity in the text, and the category or label of the named entity.\n",
    "\n",
    "For example, (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before you train, remember that apart from ner , the model has other pipeline components. These components should not get affected in training.\n",
    "\n",
    "So, disable the other pipeline components through nlp.disable_pipes() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NER model\n",
    "\n",
    "(a) To train an ner model, the model has to be looped over the example for sufficient number of iterations. If you train it for like just 5 or 6 iterations, it may not be effective.\n",
    "\n",
    "(b) Before every iteration it’s a good practice to shuffle the examples randomly throughrandom.shuffle() function .\n",
    "\n",
    "This will ensure the model does not make generalizations based on the order of the examples.\n",
    "\n",
    "(c) The training data is usually passed in batches.\n",
    "\n",
    "For each iteration , the model or ner is updated through the nlp.update()\n",
    "\n",
    "At each word, the update() it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t , it adjusts the weights so that the correct action will score higher next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 5.671148613560945}\n",
      "Losses {'ner': 6.244043747428805}\n",
      "Losses {'ner': 9.126212551142089}\n",
      "Losses {'ner': 14.664301947474087}\n",
      "Losses {'ner': 15.751911189801831}\n",
      "Losses {'ner': 6.258584780152887}\n",
      "Losses {'ner': 6.3051013308531765}\n",
      "Losses {'ner': 8.96836465296201}\n",
      "Losses {'ner': 9.972942887255158}\n",
      "Losses {'ner': 15.103250343748186}\n",
      "Losses {'ner': 3.4161520173074678}\n",
      "Losses {'ner': 8.004657640978166}\n",
      "Losses {'ner': 8.137368077315386}\n",
      "Losses {'ner': 8.14868353762131}\n",
      "Losses {'ner': 9.987013692619627}\n",
      "Losses {'ner': 0.4492399188457057}\n",
      "Losses {'ner': 4.926055098764664}\n",
      "Losses {'ner': 8.422810546170012}\n",
      "Losses {'ner': 11.918632652472212}\n",
      "Losses {'ner': 15.097795510526254}\n",
      "Losses {'ner': 0.9270217642939542}\n",
      "Losses {'ner': 3.145356021896646}\n",
      "Losses {'ner': 6.297815726981867}\n",
      "Losses {'ner': 9.724805513806643}\n",
      "Losses {'ner': 11.612874856617282}\n",
      "Losses {'ner': 3.389610820672715}\n",
      "Losses {'ner': 3.4108191625882682}\n",
      "Losses {'ner': 7.383644360548715}\n",
      "Losses {'ner': 7.3865686765020655}\n",
      "Losses {'ner': 10.82627254368758}\n",
      "Losses {'ner': 1.7731924496297324}\n",
      "Losses {'ner': 1.7913167152932488}\n",
      "Losses {'ner': 3.6862426805809605}\n",
      "Losses {'ner': 6.050210404595816}\n",
      "Losses {'ner': 6.599081688181911}\n",
      "Losses {'ner': 3.5578769003515944}\n",
      "Losses {'ner': 5.349918969314068}\n",
      "Losses {'ner': 7.342009823767704}\n",
      "Losses {'ner': 9.198582504904344}\n",
      "Losses {'ner': 9.216356872683118}\n",
      "Losses {'ner': 2.5852450910155085}\n",
      "Losses {'ner': 3.4366746753324833}\n",
      "Losses {'ner': 4.129092812026904}\n",
      "Losses {'ner': 4.14105300232638}\n",
      "Losses {'ner': 4.263647447697458}\n",
      "Losses {'ner': 0.0006037987254785548}\n",
      "Losses {'ner': 3.016972268776499}\n",
      "Losses {'ner': 5.014512667697431}\n",
      "Losses {'ner': 7.104866180602926}\n",
      "Losses {'ner': 7.6832759599298015}\n",
      "Losses {'ner': 1.9256717016506855}\n",
      "Losses {'ner': 5.38947150848351}\n",
      "Losses {'ner': 5.536661705749864}\n",
      "Losses {'ner': 8.262507798897857}\n",
      "Losses {'ner': 9.734308231941284}\n",
      "Losses {'ner': 3.644627131985999}\n",
      "Losses {'ner': 5.622524856759924}\n",
      "Losses {'ner': 6.672461811153241}\n",
      "Losses {'ner': 8.396568836001094}\n",
      "Losses {'ner': 8.396584650546716}\n",
      "Losses {'ner': 1.0986919115493947}\n",
      "Losses {'ner': 2.291521172503634}\n",
      "Losses {'ner': 3.5632736173462387}\n",
      "Losses {'ner': 5.45125509891251}\n",
      "Losses {'ner': 5.500335718899363}\n",
      "Losses {'ner': 1.1024594079956387}\n",
      "Losses {'ner': 1.231203967418196}\n",
      "Losses {'ner': 2.0569214640238442}\n",
      "Losses {'ner': 3.229313787777901}\n",
      "Losses {'ner': 3.490763871442453}\n",
      "Losses {'ner': 1.2412589476066387}\n",
      "Losses {'ner': 2.2326245447970905}\n",
      "Losses {'ner': 2.75551464418038}\n",
      "Losses {'ner': 3.1367231624204805}\n",
      "Losses {'ner': 4.3790875364680915}\n",
      "Losses {'ner': 1.6719968196290296}\n",
      "Losses {'ner': 1.741945660859173}\n",
      "Losses {'ner': 2.28689730168081}\n",
      "Losses {'ner': 2.333282429993787}\n",
      "Losses {'ner': 2.333308075693184}\n",
      "Losses {'ner': 0.18159826451319816}\n",
      "Losses {'ner': 1.4851056504620743}\n",
      "Losses {'ner': 1.4852599782554918}\n",
      "Losses {'ner': 2.4364149893268916}\n",
      "Losses {'ner': 3.3426070723825774}\n",
      "Losses {'ner': 0.07065802311595348}\n",
      "Losses {'ner': 0.8087121331380942}\n",
      "Losses {'ner': 0.8922435108189332}\n",
      "Losses {'ner': 0.8922959636007525}\n",
      "Losses {'ner': 2.448760733084165}\n",
      "Losses {'ner': 1.9837838157127408}\n",
      "Losses {'ner': 3.7053212934446833}\n",
      "Losses {'ner': 3.7125683176638518}\n",
      "Losses {'ner': 4.727819365219718}\n",
      "Losses {'ner': 5.189245543884164}\n",
      "Losses {'ner': 1.4758926597185962}\n",
      "Losses {'ner': 2.078632917392138}\n",
      "Losses {'ner': 2.9183491563614314}\n",
      "Losses {'ner': 2.9183494307735285}\n",
      "Losses {'ner': 2.918911090621647}\n",
      "Losses {'ner': 1.8042724247990805e-06}\n",
      "Losses {'ner': 3.7797982032345905}\n",
      "Losses {'ner': 3.7798081019597047}\n",
      "Losses {'ner': 3.914025753024947}\n",
      "Losses {'ner': 3.914037801879741}\n",
      "Losses {'ner': 0.3977017293323799}\n",
      "Losses {'ner': 3.4725624085654205}\n",
      "Losses {'ner': 3.472829505776656}\n",
      "Losses {'ner': 3.473133708106502}\n",
      "Losses {'ner': 3.4732059798100297}\n",
      "Losses {'ner': 1.0166957259205311}\n",
      "Losses {'ner': 1.0166968791767685}\n",
      "Losses {'ner': 1.019549762810036}\n",
      "Losses {'ner': 1.020478882188708}\n",
      "Losses {'ner': 2.908943934404998}\n",
      "Losses {'ner': 0.04860519063117863}\n",
      "Losses {'ner': 0.21041843249101777}\n",
      "Losses {'ner': 0.43790936649206375}\n",
      "Losses {'ner': 0.44384565867479525}\n",
      "Losses {'ner': 0.44451434656450944}\n",
      "Losses {'ner': 1.1582877519983796}\n",
      "Losses {'ner': 2.415910554962683}\n",
      "Losses {'ner': 2.488212692983872}\n",
      "Losses {'ner': 2.488824121910485}\n",
      "Losses {'ner': 2.4888363885593905}\n",
      "Losses {'ner': 1.131799148512561e-05}\n",
      "Losses {'ner': 0.0054522781975830225}\n",
      "Losses {'ner': 1.3569855495699013}\n",
      "Losses {'ner': 2.0632734863389564}\n",
      "Losses {'ner': 2.0632765197763225}\n",
      "Losses {'ner': 0.03896816420184407}\n",
      "Losses {'ner': 0.03948489953121054}\n",
      "Losses {'ner': 1.9591338561679328}\n",
      "Losses {'ner': 2.263852709723221}\n",
      "Losses {'ner': 2.2639624922833215}\n",
      "Losses {'ner': 1.5917985725670958}\n",
      "Losses {'ner': 1.59234435099751}\n",
      "Losses {'ner': 1.5923628148111801}\n",
      "Losses {'ner': 1.6023766059999895}\n",
      "Losses {'ner': 1.6025473937224355}\n",
      "Losses {'ner': 1.9760963493219066}\n",
      "Losses {'ner': 1.9762083611094123}\n",
      "Losses {'ner': 3.50253141641841}\n",
      "Losses {'ner': 3.5028798639248904}\n",
      "Losses {'ner': 4.954452941969047}\n",
      "Losses {'ner': 1.900538043095849e-05}\n",
      "Losses {'ner': 3.674731198405524e-05}\n",
      "Losses {'ner': 2.093721673263756}\n",
      "Losses {'ner': 2.09372169245477}\n",
      "Losses {'ner': 2.7448660303981245}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "If predictions are not up to expectations, include more training examples and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alto', 'PRODUCT'), ('Flipkart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto which I bought from Flipkart\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it your desired directory through the to_disk command.\n",
    "\n",
    "After saving, you can load the model from the directory at any point of time by passing the directory path to spacy.load() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to \\content\n"
     ]
    }
   ],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('/content/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \\content\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new labels\n",
    "\n",
    "Similar 'ner' updating is needed with Training Data. Add the label to ner through add_label() method. Next, you can use resume_training() function to return an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]\n",
    "\n",
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass sufficient examples and good number of iterations, say 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 6.135417460280541}\n",
      "Losses {'ner': 18.055886029082423}\n",
      "Losses {'ner': 26.98418468125451}\n",
      "Losses {'ner': 29.93112833611222}\n",
      "Losses {'ner': 42.66409005037997}\n",
      "Losses {'ner': 47.60032568394229}\n",
      "Losses {'ner': 52.556862453945804}\n",
      "Losses {'ner': 57.99250314309007}\n",
      "Losses {'ner': 63.61409512900268}\n",
      "Losses {'ner': 65.15030186584116}\n",
      "Losses {'ner': 72.25433349458224}\n",
      "Losses {'ner': 77.04438918883169}\n",
      "Losses {'ner': 83.47645027281031}\n",
      "Losses {'ner': 90.64281290115103}\n",
      "Losses {'ner': 1.9889164222981925}\n",
      "Losses {'ner': 6.92639906005658}\n",
      "Losses {'ner': 9.8967614361081}\n",
      "Losses {'ner': 14.314272661293769}\n",
      "Losses {'ner': 16.30601631543266}\n",
      "Losses {'ner': 19.142050187208387}\n",
      "Losses {'ner': 27.31261651880357}\n",
      "Losses {'ner': 34.23241859108691}\n",
      "Losses {'ner': 44.25706630379443}\n",
      "Losses {'ner': 49.1747580840816}\n",
      "Losses {'ner': 53.14143339348539}\n",
      "Losses {'ner': 61.481861582634345}\n",
      "Losses {'ner': 66.44031007089961}\n",
      "Losses {'ner': 73.32213563857846}\n",
      "Losses {'ner': 3.7872812948189676}\n",
      "Losses {'ner': 7.356638347862727}\n",
      "Losses {'ner': 14.315541165082223}\n",
      "Losses {'ner': 21.27128299734386}\n",
      "Losses {'ner': 26.360053868669297}\n",
      "Losses {'ner': 32.749179282861974}\n",
      "Losses {'ner': 35.3576765727134}\n",
      "Losses {'ner': 39.40600076168762}\n",
      "Losses {'ner': 48.36320856204243}\n",
      "Losses {'ner': 49.98927343167078}\n",
      "Losses {'ner': 54.30223295111465}\n",
      "Losses {'ner': 62.27424779553222}\n",
      "Losses {'ner': 71.12656784822826}\n",
      "Losses {'ner': 74.64730171506886}\n",
      "Losses {'ner': 3.837474623827802}\n",
      "Losses {'ner': 7.023237131077622}\n",
      "Losses {'ner': 11.226175330120896}\n",
      "Losses {'ner': 13.824300641856098}\n",
      "Losses {'ner': 17.950286396247975}\n",
      "Losses {'ner': 23.814348201696703}\n",
      "Losses {'ner': 28.819193520706904}\n",
      "Losses {'ner': 34.3554601743308}\n",
      "Losses {'ner': 39.058779829858395}\n",
      "Losses {'ner': 41.11634054348542}\n",
      "Losses {'ner': 49.055249913006264}\n",
      "Losses {'ner': 55.46605990675016}\n",
      "Losses {'ner': 61.759759206201124}\n",
      "Losses {'ner': 62.93088056646957}\n",
      "Losses {'ner': 7.46548193693161}\n",
      "Losses {'ner': 12.475946867838502}\n",
      "Losses {'ner': 14.715780345839448}\n",
      "Losses {'ner': 19.379613357712515}\n",
      "Losses {'ner': 21.492392465952435}\n",
      "Losses {'ner': 26.427014586995938}\n",
      "Losses {'ner': 28.474785509257345}\n",
      "Losses {'ner': 30.459554492346797}\n",
      "Losses {'ner': 33.56917686007819}\n",
      "Losses {'ner': 38.90160333357653}\n",
      "Losses {'ner': 42.15796841586871}\n",
      "Losses {'ner': 47.546773313534686}\n",
      "Losses {'ner': 50.51715795601649}\n",
      "Losses {'ner': 55.97416686172528}\n",
      "Losses {'ner': 3.5426370768836932}\n",
      "Losses {'ner': 9.035783693849226}\n",
      "Losses {'ner': 16.996129558145185}\n",
      "Losses {'ner': 17.399982804592582}\n",
      "Losses {'ner': 21.31895661719318}\n",
      "Losses {'ner': 24.02514165789762}\n",
      "Losses {'ner': 29.16991982669788}\n",
      "Losses {'ner': 34.24550335611639}\n",
      "Losses {'ner': 36.72093247752127}\n",
      "Losses {'ner': 39.750699689047906}\n",
      "Losses {'ner': 42.839219865756604}\n",
      "Losses {'ner': 42.84561978149213}\n",
      "Losses {'ner': 49.25083923790589}\n",
      "Losses {'ner': 55.31549952234127}\n",
      "Losses {'ner': 2.7445076965860835}\n",
      "Losses {'ner': 7.330554314471669}\n",
      "Losses {'ner': 13.405413314282782}\n",
      "Losses {'ner': 18.246378570843376}\n",
      "Losses {'ner': 21.00167666262962}\n",
      "Losses {'ner': 23.09337816906293}\n",
      "Losses {'ner': 27.616286793044424}\n",
      "Losses {'ner': 32.706600760361766}\n",
      "Losses {'ner': 36.15724442106614}\n",
      "Losses {'ner': 38.87151251939633}\n",
      "Losses {'ner': 42.33842188066103}\n",
      "Losses {'ner': 46.070172033898984}\n",
      "Losses {'ner': 50.59298455847659}\n",
      "Losses {'ner': 56.16666128805139}\n",
      "Losses {'ner': 2.9328090430267366}\n",
      "Losses {'ner': 5.053547227048057}\n",
      "Losses {'ner': 8.838060450149442}\n",
      "Losses {'ner': 11.886917634276415}\n",
      "Losses {'ner': 16.871712757615114}\n",
      "Losses {'ner': 23.594727976239028}\n",
      "Losses {'ner': 26.493683334431353}\n",
      "Losses {'ner': 29.551258087382394}\n",
      "Losses {'ner': 34.17946633486383}\n",
      "Losses {'ner': 36.4293552119135}\n",
      "Losses {'ner': 39.631460658632136}\n",
      "Losses {'ner': 39.74428134447328}\n",
      "Losses {'ner': 42.84927193367287}\n",
      "Losses {'ner': 49.812091260341845}\n",
      "Losses {'ner': 2.5712037570774555}\n",
      "Losses {'ner': 7.240999836474657}\n",
      "Losses {'ner': 15.390614587813616}\n",
      "Losses {'ner': 20.465302353724837}\n",
      "Losses {'ner': 20.473544779481017}\n",
      "Losses {'ner': 24.536050009788596}\n",
      "Losses {'ner': 27.289035161040374}\n",
      "Losses {'ner': 34.2902789311629}\n",
      "Losses {'ner': 39.09018169682531}\n",
      "Losses {'ner': 44.270017326596644}\n",
      "Losses {'ner': 45.78610100840888}\n",
      "Losses {'ner': 50.56419438553712}\n",
      "Losses {'ner': 53.51935869105364}\n",
      "Losses {'ner': 59.430226445423614}\n",
      "Losses {'ner': 4.920932404696941}\n",
      "Losses {'ner': 8.542504986864515}\n",
      "Losses {'ner': 14.456327928346582}\n",
      "Losses {'ner': 17.46570686076302}\n",
      "Losses {'ner': 24.87515497591812}\n",
      "Losses {'ner': 32.352780822780915}\n",
      "Losses {'ner': 34.38721761253055}\n",
      "Losses {'ner': 40.20395396310141}\n",
      "Losses {'ner': 47.179153245270754}\n",
      "Losses {'ner': 51.66110027826744}\n",
      "Losses {'ner': 54.127964653665}\n",
      "Losses {'ner': 57.550144978690696}\n",
      "Losses {'ner': 60.343656772624854}\n",
      "Losses {'ner': 62.2820460905059}\n",
      "Losses {'ner': 5.455496720969677}\n",
      "Losses {'ner': 10.497991466138046}\n",
      "Losses {'ner': 13.123656198673416}\n",
      "Losses {'ner': 17.346978162939195}\n",
      "Losses {'ner': 19.339893866505008}\n",
      "Losses {'ner': 22.91734290047316}\n",
      "Losses {'ner': 25.139569650113117}\n",
      "Losses {'ner': 25.209752326754824}\n",
      "Losses {'ner': 29.279065558144794}\n",
      "Losses {'ner': 32.157509053149624}\n",
      "Losses {'ner': 33.23732714206744}\n",
      "Losses {'ner': 34.19738833464521}\n",
      "Losses {'ner': 37.66957497268754}\n",
      "Losses {'ner': 43.9930657711609}\n",
      "Losses {'ner': 3.696151375770569}\n",
      "Losses {'ner': 7.359835646115243}\n",
      "Losses {'ner': 20.774117968045175}\n",
      "Losses {'ner': 24.637702441781585}\n",
      "Losses {'ner': 27.38252679231664}\n",
      "Losses {'ner': 31.906750668735185}\n",
      "Losses {'ner': 33.575054299981275}\n",
      "Losses {'ner': 34.548712946486376}\n",
      "Losses {'ner': 41.24414065562905}\n",
      "Losses {'ner': 44.482561948590046}\n",
      "Losses {'ner': 50.49074356632349}\n",
      "Losses {'ner': 51.9115334451389}\n",
      "Losses {'ner': 55.654482902127896}\n",
      "Losses {'ner': 60.40832734929104}\n",
      "Losses {'ner': 0.9742045403940978}\n",
      "Losses {'ner': 1.9614772436357129}\n",
      "Losses {'ner': 7.048964823712353}\n",
      "Losses {'ner': 8.014656544711485}\n",
      "Losses {'ner': 10.921123952653303}\n",
      "Losses {'ner': 14.874670269947543}\n",
      "Losses {'ner': 19.693061666378796}\n",
      "Losses {'ner': 25.3382522097575}\n",
      "Losses {'ner': 29.93680348054322}\n",
      "Losses {'ner': 37.06429249600323}\n",
      "Losses {'ner': 46.10848124504837}\n",
      "Losses {'ner': 48.93466638580712}\n",
      "Losses {'ner': 51.800794106346345}\n",
      "Losses {'ner': 53.743466987262124}\n",
      "Losses {'ner': 2.1906465537172153}\n",
      "Losses {'ner': 4.359611297864376}\n",
      "Losses {'ner': 10.775301229311935}\n",
      "Losses {'ner': 13.591181434008377}\n",
      "Losses {'ner': 21.438859815841624}\n",
      "Losses {'ner': 27.045875410681674}\n",
      "Losses {'ner': 35.27910725319953}\n",
      "Losses {'ner': 37.67946305142823}\n",
      "Losses {'ner': 39.788988284775314}\n",
      "Losses {'ner': 45.87458170675882}\n",
      "Losses {'ner': 49.02908656471618}\n",
      "Losses {'ner': 51.36130121574888}\n",
      "Losses {'ner': 59.5373795334358}\n",
      "Losses {'ner': 62.656751429429505}\n",
      "Losses {'ner': 4.480079650878906}\n",
      "Losses {'ner': 6.37264536076691}\n",
      "Losses {'ner': 10.177614908549003}\n",
      "Losses {'ner': 15.45349055621773}\n",
      "Losses {'ner': 23.41808891389519}\n",
      "Losses {'ner': 23.441493196787633}\n",
      "Losses {'ner': 30.149767049730144}\n",
      "Losses {'ner': 34.876536450490676}\n",
      "Losses {'ner': 39.34420792399396}\n",
      "Losses {'ner': 39.35628272233953}\n",
      "Losses {'ner': 44.346685338979626}\n",
      "Losses {'ner': 49.41835282437711}\n",
      "Losses {'ner': 53.64810743794584}\n",
      "Losses {'ner': 55.60244298434014}\n",
      "Losses {'ner': 4.773640334839001}\n",
      "Losses {'ner': 10.685256627737544}\n",
      "Losses {'ner': 11.759042913559824}\n",
      "Losses {'ner': 17.30586988838877}\n",
      "Losses {'ner': 20.111122741766394}\n",
      "Losses {'ner': 25.24855137610075}\n",
      "Losses {'ner': 26.961471087992777}\n",
      "Losses {'ner': 29.297379016754803}\n",
      "Losses {'ner': 30.296863129857343}\n",
      "Losses {'ner': 36.94972693201282}\n",
      "Losses {'ner': 39.42297432877251}\n",
      "Losses {'ner': 44.88557408776262}\n",
      "Losses {'ner': 52.41543088879564}\n",
      "Losses {'ner': 57.189453988830905}\n",
      "Losses {'ner': 3.2178475006949157}\n",
      "Losses {'ner': 8.911354783620709}\n",
      "Losses {'ner': 18.323153135385155}\n",
      "Losses {'ner': 20.00538981239174}\n",
      "Losses {'ner': 20.987341018036886}\n",
      "Losses {'ner': 25.31572345159293}\n",
      "Losses {'ner': 28.497666874940478}\n",
      "Losses {'ner': 32.06689584608466}\n",
      "Losses {'ner': 35.51888090236389}\n",
      "Losses {'ner': 37.44723029505258}\n",
      "Losses {'ner': 37.448401681294854}\n",
      "Losses {'ner': 41.40345869833436}\n",
      "Losses {'ner': 47.51840774589982}\n",
      "Losses {'ner': 52.73112292928454}\n",
      "Losses {'ner': 5.521322200693248}\n",
      "Losses {'ner': 6.769024614978434}\n",
      "Losses {'ner': 9.299482280698044}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 15.070842868026716}\n",
      "Losses {'ner': 17.41177810350598}\n",
      "Losses {'ner': 20.929445350954097}\n",
      "Losses {'ner': 25.880433935083452}\n",
      "Losses {'ner': 32.75067881063372}\n",
      "Losses {'ner': 37.85085251409017}\n",
      "Losses {'ner': 40.69515348538933}\n",
      "Losses {'ner': 46.009373735042196}\n",
      "Losses {'ner': 53.27040446996633}\n",
      "Losses {'ner': 57.52870198565014}\n",
      "Losses {'ner': 65.76614764546557}\n",
      "Losses {'ner': 4.269399568118388}\n",
      "Losses {'ner': 9.889373769663507}\n",
      "Losses {'ner': 13.241571735736215}\n",
      "Losses {'ner': 15.619898533834203}\n",
      "Losses {'ner': 24.24267604007764}\n",
      "Losses {'ner': 24.254182549579127}\n",
      "Losses {'ner': 28.367168055854563}\n",
      "Losses {'ner': 31.062403062631347}\n",
      "Losses {'ner': 36.04521516179011}\n",
      "Losses {'ner': 37.667203489265376}\n",
      "Losses {'ner': 40.24817592875843}\n",
      "Losses {'ner': 47.3054819800127}\n",
      "Losses {'ner': 50.601195993291185}\n",
      "Losses {'ner': 53.497107728657284}\n",
      "Losses {'ner': 3.7994599402516087}\n",
      "Losses {'ner': 7.913474875472957}\n",
      "Losses {'ner': 8.871293293435656}\n",
      "Losses {'ner': 13.129610070431}\n",
      "Losses {'ner': 18.005597121818255}\n",
      "Losses {'ner': 21.45686332244918}\n",
      "Losses {'ner': 23.4375586244671}\n",
      "Losses {'ner': 31.926307210912682}\n",
      "Losses {'ner': 37.89031499273119}\n",
      "Losses {'ner': 42.03998352406256}\n",
      "Losses {'ner': 43.98633588726284}\n",
      "Losses {'ner': 50.42822582895519}\n",
      "Losses {'ner': 53.84277981471837}\n",
      "Losses {'ner': 56.01318739414677}\n",
      "Losses {'ner': 0.23327851109661424}\n",
      "Losses {'ner': 2.349779745307842}\n",
      "Losses {'ner': 5.1872203110019655}\n",
      "Losses {'ner': 8.204827486281374}\n",
      "Losses {'ner': 10.281333395949218}\n",
      "Losses {'ner': 15.339612284413192}\n",
      "Losses {'ner': 19.17351095654874}\n",
      "Losses {'ner': 23.802547120577287}\n",
      "Losses {'ner': 25.94348077863185}\n",
      "Losses {'ner': 30.528303948377356}\n",
      "Losses {'ner': 37.58676575828656}\n",
      "Losses {'ner': 41.44353610625956}\n",
      "Losses {'ner': 42.80034679566859}\n",
      "Losses {'ner': 46.02670013503928}\n",
      "Losses {'ner': 7.4740142822265625}\n",
      "Losses {'ner': 8.700487183210498}\n",
      "Losses {'ner': 12.707028264107574}\n",
      "Losses {'ner': 18.540835578335244}\n",
      "Losses {'ner': 26.068862248552758}\n",
      "Losses {'ner': 30.053955034934287}\n",
      "Losses {'ner': 30.053968669621845}\n",
      "Losses {'ner': 34.19224637525878}\n",
      "Losses {'ner': 42.37966403260618}\n",
      "Losses {'ner': 42.397540021604755}\n",
      "Losses {'ner': 46.53808376091615}\n",
      "Losses {'ner': 49.9353200292272}\n",
      "Losses {'ner': 49.935744940161925}\n",
      "Losses {'ner': 53.38895897783249}\n",
      "Losses {'ner': 2.8930259824938958}\n",
      "Losses {'ner': 12.345428454989928}\n",
      "Losses {'ner': 14.289150488478185}\n",
      "Losses {'ner': 17.775981536064933}\n",
      "Losses {'ner': 20.332564982198733}\n",
      "Losses {'ner': 27.686597979107887}\n",
      "Losses {'ner': 27.688507951669862}\n",
      "Losses {'ner': 31.856093887690918}\n",
      "Losses {'ner': 37.176388987791256}\n",
      "Losses {'ner': 44.33779805128154}\n",
      "Losses {'ner': 46.29656071885995}\n",
      "Losses {'ner': 53.8710554375881}\n",
      "Losses {'ner': 56.3620819115082}\n",
      "Losses {'ner': 58.62783512434055}\n",
      "Losses {'ner': 0.00020967370136304453}\n",
      "Losses {'ner': 5.289966165481644}\n",
      "Losses {'ner': 7.918031957019309}\n",
      "Losses {'ner': 7.939904869885652}\n",
      "Losses {'ner': 15.224150657018583}\n",
      "Losses {'ner': 20.904468804871527}\n",
      "Losses {'ner': 22.8424551142206}\n",
      "Losses {'ner': 26.393602089374454}\n",
      "Losses {'ner': 32.3221462654184}\n",
      "Losses {'ner': 34.84873721388534}\n",
      "Losses {'ner': 38.83597847083575}\n",
      "Losses {'ner': 40.92930668717718}\n",
      "Losses {'ner': 46.371311172232986}\n",
      "Losses {'ner': 52.24103786007736}\n",
      "Losses {'ner': 5.106609736827522}\n",
      "Losses {'ner': 10.808427525011211}\n",
      "Losses {'ner': 12.068395693551722}\n",
      "Losses {'ner': 20.485571353898123}\n",
      "Losses {'ner': 25.068299683740925}\n",
      "Losses {'ner': 30.96389392297585}\n",
      "Losses {'ner': 35.201537401996184}\n",
      "Losses {'ner': 37.50743839699732}\n",
      "Losses {'ner': 38.7068460499491}\n",
      "Losses {'ner': 40.03677580715017}\n",
      "Losses {'ner': 40.04653380837632}\n",
      "Losses {'ner': 46.445612710233455}\n",
      "Losses {'ner': 50.08609063970149}\n",
      "Losses {'ner': 52.86305326942462}\n",
      "Losses {'ner': 4.620006332115736}\n",
      "Losses {'ner': 9.968512231469504}\n",
      "Losses {'ner': 15.925855273366324}\n",
      "Losses {'ner': 21.26081553554104}\n",
      "Losses {'ner': 23.250504156399074}\n",
      "Losses {'ner': 28.080221118893228}\n",
      "Losses {'ner': 32.04196164898922}\n",
      "Losses {'ner': 37.76997982111816}\n",
      "Losses {'ner': 40.29361863675995}\n",
      "Losses {'ner': 44.77221663794301}\n",
      "Losses {'ner': 45.33303128198718}\n",
      "Losses {'ner': 50.63966713880179}\n",
      "Losses {'ner': 51.69324991460578}\n",
      "Losses {'ner': 55.28155148026714}\n",
      "Losses {'ner': 4.423350081029467}\n",
      "Losses {'ner': 6.387791406836014}\n",
      "Losses {'ner': 10.577826049651604}\n",
      "Losses {'ner': 15.787484301494088}\n",
      "Losses {'ner': 24.627614690230814}\n",
      "Losses {'ner': 27.0941204264075}\n",
      "Losses {'ner': 30.41237645067875}\n",
      "Losses {'ner': 33.28215374061645}\n",
      "Losses {'ner': 35.44766661255224}\n",
      "Losses {'ner': 35.46793485738783}\n",
      "Losses {'ner': 38.97346341537525}\n",
      "Losses {'ner': 43.28668679653083}\n",
      "Losses {'ner': 46.63416686014125}\n",
      "Losses {'ner': 51.43666601440896}\n",
      "Losses {'ner': 2.921364856825676}\n",
      "Losses {'ner': 8.850782604713459}\n",
      "Losses {'ner': 9.907053090952104}\n",
      "Losses {'ner': 13.739573374710744}\n",
      "Losses {'ner': 16.72475501181907}\n",
      "Losses {'ner': 16.732254043292215}\n",
      "Losses {'ner': 16.737226608026262}\n",
      "Losses {'ner': 19.328335751872814}\n",
      "Losses {'ner': 27.22624430453901}\n",
      "Losses {'ner': 30.764321196277464}\n",
      "Losses {'ner': 33.66024278451253}\n",
      "Losses {'ner': 36.01022666266114}\n",
      "Losses {'ner': 36.01031126929514}\n",
      "Losses {'ner': 42.88367367085369}\n",
      "Losses {'ner': 2.2865281177219003}\n",
      "Losses {'ner': 3.2536276420842114}\n",
      "Losses {'ner': 7.062014278516017}\n",
      "Losses {'ner': 12.216842797145091}\n",
      "Losses {'ner': 15.876738714757039}\n",
      "Losses {'ner': 20.918129797804028}\n",
      "Losses {'ner': 24.060590920827195}\n",
      "Losses {'ner': 26.46218644582146}\n",
      "Losses {'ner': 30.542117006001735}\n",
      "Losses {'ner': 34.3499906722882}\n",
      "Losses {'ner': 35.63791503886297}\n",
      "Losses {'ner': 39.921280553923694}\n",
      "Losses {'ner': 44.03897003522549}\n",
      "Losses {'ner': 47.4103504458849}\n",
      "Losses {'ner': 3.6872000180077205}\n",
      "Losses {'ner': 3.68848816371416}\n",
      "Losses {'ner': 4.665313901502818}\n",
      "Losses {'ner': 10.528799186583788}\n",
      "Losses {'ner': 12.912058345165283}\n",
      "Losses {'ner': 15.083685933007805}\n",
      "Losses {'ner': 17.610977687031998}\n",
      "Losses {'ner': 20.164465436533405}\n",
      "Losses {'ner': 27.434063860968067}\n",
      "Losses {'ner': 30.038055491484315}\n",
      "Losses {'ner': 33.967655120936556}\n",
      "Losses {'ner': 33.968895364764464}\n",
      "Losses {'ner': 38.720690426920484}\n",
      "Losses {'ner': 42.623659206973954}\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "  sizes = compounding(1.0, 4.0, 1.001)\n",
    "  # Training for 30 iterations     \n",
    "  for itn in range(30):\n",
    "    # shuffle examples before training\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "    # ictionary to store losses\n",
    "    losses = {}\n",
    "    for batch in batches:\n",
    "      texts, annotations = zip(*batch)\n",
    "      # Calling update() over the iteration\n",
    "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "      print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to \\content\n",
      "Loading from \\content\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('/content/')\n",
    "\n",
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "  output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \\content\n",
      "No Entities found!!\n"
     ]
    }
   ],
   "source": [
    "# Loading the model from the directory\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "doc2 = nlp2(' Idli is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "  print(ent.label_, ent.text)\n",
    "else:\n",
    "  print(\"No Entities found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
