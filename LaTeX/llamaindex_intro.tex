%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Background}

\begin{itemize}
\item LLMs (Large Language Models) are pre-trained on large amounts of publicly available data. Sometimes having a cut-off date, say, Nov 2021.
\item How do we best augment LLMs with our own private data?
\item But how to ingest private knowledge? Many ways \ldots
\item How to use them with private data: Few shots, RAG, Fine-Tuning. 
\item Fine-tuning: adding last layer, and retraining the weights, but downsides are:
	\begin{itemize}
	\item Data preparation effort
	\item Lack of transparency
	\item May not work well
	\item High upfront cost
	\end{itemize}	
\item Most effective-middle-ground: RAG (Retriveal Augmented Generation) ie In-context learning - putting context into the prompt
\end{itemize}	


\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{In-context learning}

Key challenges:

\begin{itemize}
\item How to retrieve the right context for the prompt?
\item How to deal with long context? 
\item How to deal with source data that is potentially very large? (GB’s, TB’s) 
\item How to trade-off between: Accuracy, Latency, Cost
\end{itemize}	

Solution \ldots LlamaIndex

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is LlamaIndex?}

\begin{itemize}
\item Calling an LLM API is easy. Setting up a software system that can extract insights from your private data is harder.
\item LlamaIndex is the advanced data framework for your LLM applications
\item Data Management: Data ingestion, data parsing/slicing, data storage/indexing.
\item Data Querying: Data retrieval, response synthesis, multi-step interactions over your data.
\item LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. 
\end{itemize}	


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{From Jerry Liu, co-founder, LlamaIndex?}

\begin{itemize}
\item LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the “neural” compute unit at the core of a new age of AI-enabled software.
\item Yet, LLMs inherently have no awareness of your own data.
\item No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune.
\item If we could offer a toolkit to help set up the data architecture for LLM apps, then we could enable anyone to build LLM-powered knowledge workers and transform the way that software is written over private data.
\end{itemize}	

LlamaIndex became viewed as a critical data orchestration component of the emerging LLM software landscape.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{So, LlamaIndex: A interface between your data and LLMs }


\begin{itemize}
\item Data management + query engine
\item Goal is to make this interface fast, cheap, efficient, and performant 
\item Components:
	\begin{itemize}
	\item Data Ingestion (LlamaHub): Connect your existing data sources and data formats (API’s, PDF’s, docs, SQL, etc.)
	\item Data Structure: Store and index your data in different data structures such as lists, trees, graphs, for different use cases. 
	\item Query Interface: Feed in an input prompt and obtain a knowledge-augmented output.
	\end{itemize}	

\end{itemize}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What it has?}


\begin{itemize}
\item 100+ data loaders
\item 13+ vector database providers
\item Integration with observability and experimentation frameworks (e.g. prompt tracking and system tracing)
\item Integration as a ChatGPT Retrieval Plugin or with Poe
\end{itemize}	

Vision: unlocking LLM capabilities on top of your data.
\end{frame}
