%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Conclusion}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Conclusion}

\begin{itemize}
\item RL helps us to discover which action could yield the highest reward for a longer time. 
\item Realistic environments can have partial observability and be non-stationary as well. 
\item It isn't very useful to apply when you have hands-on enough data to solve the problem using supervised learning. \item The main challenge of this method is that parameters could affect the speed of the learning.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}\frametitle{If you have grasped RL by now, then, Test}
% As you have come so far, lets see if you can solve the following:

% \begin{itemize}
% % \item How would you find a minimum of $y=e^(2 - x) + x^2 - x/2$?
% % \item Transformers: can you have 5 attention heads with a sequence length equal to 100 and embedding dimension to 512, and why?
% \item Please explain why the Bellman Equation is important and explain how you have used it in your work.
% \item DQN RL: how can you use DQN on an environment with 100 unique states, and actions taking values on the interval $[-0.5, 0.5]$?
% \end{itemize}

% {\tiny (Ref: ``Reinforcement Learning Expert'' - MSBAI)}

% If, there is a long way to go \ldots  Next are some of the learning resources \ldots
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{How to get started?}

\begin{itemize}
\item Reinforcement Learning-An Introduction, a book by the father of Reinforcement Learning- Richard Sutton and his doctoral advisor Andrew Barto. An online draft available.
\item Course by David Silver including video lectures.
\item Technical tutorial by Pieter Abbeel and John Schulman (Open AI/ Berkeley AI Research Lab).
\item "Deep Reinforcement Learning: Pong from Pixels" - Andrej Karpathy blog
\end{itemize}

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Learning Resources}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{rl24}
% \end{center}
% {\tiny (Ref: Newbie's Guide to Study Reinforcement Learning - Towards Data Science)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}\frametitle{RL for AI Safety}

% \begin{itemize}
% \item Deep reinforcement learning to play role in AI Safety
% \item To prepare: not yet a standard deep RL textbook, most of the knowledge is locked up in either papers; takes a long time to digest.
% \item Deep RL algorithms are painful due to obscure papers and public implementations.
% \item Open AI Spinning Up is a useful Library.
% \item Has algorithms like DDPG and Q-Learning are off-policy, so they are able to reuse old data very efficiently. 
% \item Has Vanilla Policy Gradient, a on-policy algorithm: that is, they don't use old data, which makes them weaker on sample efficiency. 
% \end{itemize}

% {\tiny (Ref: ``Spinning Up'' - OpenAI)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Recommended Reading: Deep RL Algorithms}

% \begin{itemize}
% \item A2C / A3C: Mnih et al, 2016 (\url{https://arxiv.org/abs/1602.01783})
% \item PPO: Schulman et al, 2017 (\url{https://arxiv.org/abs/1707.06347})
% \item TRPO: Schulman et al, 2015 (\url{https://arxiv.org/abs/1502.05477})
% \item DQN: Mnih et al, 2013 (\url{https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf})
% \item C51: Bellemare et al, 2017 (\url{https://arxiv.org/abs/1707.06887})
% \item QR-DQN: Dabney et al, 2017 (\url{https://arxiv.org/abs/1710.10044})
% \item DDPG: Lillicrap et al, 2015 (\url{https://arxiv.org/abs/1509.02971})
% \item SVG: Heess et al, 2015 (\url{https://arxiv.org/abs/1510.09142})
% \item I2A: Weber et al, 2017 (\url{https://arxiv.org/abs/1707.06203})
% \item MBMF: Nagabandi et al, 2017 (\url{https://sites.google.com/view/mbmf})
% \item AlphaZero: Silver et al, 2017 (\url{https://arxiv.org/abs/1712.01815})
% \end{itemize}
% {\tiny (Ref: Intro to RL - Joshua Achiam)}


% \end{frame}
